🧪 How to Test for Independence
1. For Categorical Data
Use a Chi-square test of independence.

Make a contingency table of your variables.

Run a chi-square test to see if observed frequencies differ from expected.

If the p-value < 0.05 → likely not independent.

2. For Numeric (Continuous) Data
Use correlation or other methods depending on the context:

Pearson correlation coefficient (r):

If 
𝑟
r is close to 0, variables might be independent.

But beware: no correlation ≠ independence always.

Mutual information:

Measures any kind of dependency (linear or not).

If mutual information = 0 → they’re independent.

Scatter plots:

Visual check: if no pattern at all, possibly independent.

Statistical tests:

For time series → use autocorrelation to test for independence in time.

Permutation tests or independence tests like the HSIC (Hilbert-Schmidt Independence Criterion) for more advanced cases.

1. Chi-Square Test for Independence
Use this to check if there's a relationship between consecutive values.

Example:
Generate a long sequence (like 1, 2, 1, 1, 2, ...). Then:

Count how many times 1 is followed by 1, 1 by 2, 2 by 1, and 2 by 2.

Compare observed vs expected counts using a chi-square test.

If the results match what you’d expect under independence (i.e. 50/50 follow patterns), the test will confirm that.

2. Runs Test (Wald–Wolfowitz Runs Test)
This test looks at the number of runs (consecutive repeated values).

Example:

Sequence: 1, 1, 2, 2, 1, 2, 2, 2, 1

Runs: [1,1], [2,2], [1], [2,2,2], [1] = 5 runs

The number of runs in a truly random binary sequence has a known distribution. If your actual number of runs is too high or low, something non-random may be happening.

3. Autocorrelation
Check if values are correlated with earlier ones.

Compute correlation between value at t and t+1, t+2, etc.

For a random sequence, autocorrelations should be ~0.

4. Entropy Measures
High entropy = high unpredictability.

You can estimate entropy of your sequence (e.g., via Shannon entropy) — it should be near 1 bit per value for a binary sequence if it’s fully random.


Mutual Information (MI) - Explanation

Entropy (information theory)


